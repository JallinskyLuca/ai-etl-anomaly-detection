{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-01",
   "metadata": {},
   "source": "### üìò Model Training & Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "<style>\n",
    ".outer-div {\n",
    "  text-align: left; /* Ensures content is left-justified */\n",
    "  padding-left: 10px; /* Pads the content by 10px from the left */\n",
    "  padding-bottom: 10px; /* Pads the content by 10px from the bottom */\n",
    "}\n",
    "\n",
    ".inner-div {\n",
    "  width: 75%; /* Sets the width of the inner div to 75% of its parent */\n",
    "  margin-left: 0; /* Ensures left justification if no other margin is applied */\n",
    "  padding: 10px; /* Applies 10px padding on all sides */\n",
    "  border-left: 1px solid #485c83; /* Left border: 1px solid with color #485c83 */\n",
    "  color: #b8bbbf; /* Sets the font color */\n",
    "  background-color: #303135; /* Sets the background color */\n",
    "}\n",
    "</style>\n",
    "<div class=\"outer-div\">\n",
    "<div class=\"inner-div\">\n",
    "<b>Notebook Summary</b><br>\n",
    "Goal: Train and evaluate baseline ML models<br>\n",
    "Author: Dennis Fashimpaur<br>\n",
    "Date: 2025-11-26<br>\n",
    "Features: numeric-only, standardized<br>\n",
    "Models: Logistic Regression, Random Forest<br>\n",
    "Evaluation: Accuracy, Precision, Recall, F1, ROC-AUC\n",
    "</div></div>\n",
    "\n",
    "This notebook performs the following tasks:\n",
    "* Loads unified dataset\n",
    "* Drops duplicate `Amount` column\n",
    "* Feature/label splitting\n",
    "* Train/test split\n",
    "* Standardization for Logistic Regression\n",
    "* Trains Logistic Regression\n",
    "* Trains Random Forest\n",
    "* Evaluates models (precision, recall, F1, ROC-AUC)\n",
    "* Displays Confusion matrices\n",
    "* Compares models\n",
    "* Plots ROC curves and Random Forest feature importance\n",
    "* Saves trained models and results\n",
    "\n",
    "### üß≠ Imports & Setup"
   ],
   "id": "f2c84f04e8ef1019"
  },
  {
   "cell_type": "code",
   "id": "code-imports",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, \\\n",
    "    roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.data_loader import load_csv\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 150)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "### üìä Load Unified Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-load-data",
   "metadata": {},
   "source": [
    "df = load_csv('../data/processed/unified_dataset.csv')\n",
    "# Drop the redundant 'Amount' column per Option A\n",
    "if 'Amount' in df.columns:\n",
    "    df = df.drop(columns=['Amount'])\n",
    "print(\"üóÑÔ∏è Loaded unified dataset with shape:\", df.shape)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "split-labels",
   "metadata": {},
   "source": [
    "### ‰∑ñ Split Features & Labels\n",
    "We keep only numeric features for modeling; the target column is `label` (1 = anomaly/fraud, 0 = normal)."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-split-labels",
   "metadata": {},
   "source": [
    "X = df.drop(columns=['label'])\n",
    "X = X.select_dtypes(include=['number'])\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "scale",
   "metadata": {},
   "source": [
    "### üîß Standardize Features (for Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale-def",
   "metadata": {},
   "source": [
    "Standardization rescales numeric features to have **mean 0 and standard deviation 1**.\n",
    "- Helps Logistic Regression converge faster.\n",
    "- Makes coefficient values more comparable across features."
   ]
  },
  {
   "cell_type": "code",
   "id": "scale-code",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eval-helper",
   "metadata": {},
   "source": [
    "### üí° Evaluation Helper"
   ]
  },
  {
   "cell_type": "code",
   "id": "eval-helper-code",
   "metadata": {},
   "source": [
    "def evaluate_model(y_test, y_pred, y_prob):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_prob)\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "logreg-train",
   "metadata": {},
   "source": [
    "### üìà Logistic Regression ‚Äî Overview & Train\n",
    "Logistic Regression is a **supervised binary classification algorithm** that models the probability of the positive class (fraud/anomaly) using a logistic (sigmoid) function applied to a linear combination of features."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-logreg-train",
   "metadata": {},
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = logreg.predict(X_test_scaled)\n",
    "y_prob_lr = logreg.predict_proba(X_test_scaled)[:, 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "logreg-eval",
   "metadata": {},
   "source": [
    "### üìà Logistic Regression ‚Äî Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-logreg-eval",
   "metadata": {},
   "source": [
    "lr_metrics = evaluate_model(y_test, y_pred_lr, y_prob_lr)\n",
    "pd.DataFrame([lr_metrics])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-confusion-lr",
   "metadata": {},
   "source": [
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Logistic Regression ‚Äî Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "rf-train",
   "metadata": {},
   "source": [
    "### üå≤ Random Forest ‚Äî Overview & Train\n",
    "Random Forest is an **ensemble of decision trees**:\n",
    "- Each tree is trained on a random subset (bootstrap sample) of data.\n",
    "- Splits consider a random subset of features.\n",
    "- Final prediction is majority vote.\n",
    "- Provides feature importance to understand which features contribute most."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-rf-train",
   "metadata": {},
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_depth=12, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_prob_rf = rf.predict_proba(X_test)[:, 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "rf-eval",
   "metadata": {},
   "source": [
    "### üå≤ Random Forest ‚Äî Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-rf-eval",
   "metadata": {},
   "source": [
    "rf_metrics = evaluate_model(y_test, y_pred_rf, y_prob_rf)\n",
    "pd.DataFrame([rf_metrics])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "code-confusion-rf",
   "metadata": {},
   "source": [
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('Random Forest ‚Äî Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "roc-curves",
   "metadata": {},
   "source": [
    "### üìä ROC Curves ‚Äî Model Comparison\n",
    "ROC curve visualizes the trade-off between **True Positive Rate (Recall)** and **False Positive Rate** for different thresholds.\n",
    "- Area Under Curve (AUC) shows discrimination ability: 1 = perfect, 0.5 = random."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-roc",
   "metadata": {},
   "source": [
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fpr_lr, tpr_lr, color='blue', label=f'LogReg (AUC = {roc_auc_lr:.3f})')\n",
    "plt.plot(fpr_rf, tpr_rf, color='green', label=f'Random Forest (AUC = {roc_auc_rf:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves ‚Äî Model Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "rf-feature-importance",
   "metadata": {},
   "source": [
    "### üß© Random Forest Feature Importance\n",
    "Feature importance shows which features contribute most to the Random Forest's decisions.\n",
    "- Higher importance = more influence on predictions."
   ]
  },
  {
   "cell_type": "code",
   "id": "code-rf-feature-importance",
   "metadata": {},
   "source": [
    "importances = rf.feature_importances_\n",
    "feat_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': importances}).sort_values(by='importance',\n",
    "                                                                                                       ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feat_importance_df)\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "### üÜö Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-comparison",
   "metadata": {},
   "source": [
    "comparison = pd.DataFrame([\n",
    "    {'model': 'Logistic Regression', **lr_metrics},\n",
    "    {'model': 'Random Forest', **rf_metrics}\n",
    "])\n",
    "comparison"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "save-models",
   "metadata": {},
   "source": [
    "### üíæ Save Models & Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-save-models",
   "metadata": {},
   "source": [
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../data/results', exist_ok=True)\n",
    "\n",
    "# Save trained models\n",
    "joblib.dump(logreg, '../models/logistic_regression.pkl')\n",
    "joblib.dump(rf, '../models/random_forest.pkl')\n",
    "\n",
    "# Save comparison results\n",
    "comparison.to_csv('../data/results/model_results.csv', index=False)\n",
    "\n",
    "print('üíø Saved models and results successfully.')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
